{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(folder_path)\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist\")\n",
    "\n",
    "delete_folder('./runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined constants\n",
    "'''\n",
    "n = 55000  # The size of training set\n",
    "scale = 1e-2 # the scale factor for loss function\n",
    "damp = 1 # the damp factor to add L2 regularization in loss function\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function without scaling and regularization\n",
    "device = 'cpu'\n",
    "iteration = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, we would load training dataset and testing dataset from MNIST. We assign the training set size $n=55000$, which is accorded to the paper Understanding Black-box Predictions via Influence Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # Normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "train_dataset_all = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transform, )\n",
    "\n",
    "train_indices = torch.randperm(len(train_dataset_all))[:n]\n",
    "train_dataset = Subset(train_dataset_all, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_list = []\n",
    "all_labels_list = []\n",
    "for i in range(len(train_dataset)):\n",
    "    data, label = train_dataset[i]\n",
    "    all_data_list.append(data)         # img shape: [1, 28, 28]\n",
    "    all_labels_list.append(label)\n",
    "\n",
    "train_data = torch.stack(all_data_list, dim=0).to(device)  # shape [n, 1, 28, 28]\n",
    "train_labels = torch.tensor(all_labels_list).to(device)        # shape [n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Logistic Regression Model Definition\n",
    "\n",
    "Below we defined the logistic regression model with training and testing method using L-BFGS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, params=None):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if params:\n",
    "            self.linear.weight = nn.Parameter(params['weight'])\n",
    "            self.linear.bias = nn.Parameter(params['bias'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        # outputs = torch.nn.functional.softmax(self.linear(x))\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs(model, criterion, train_data, train_labels, writer=None, t=20, leave_out_index=None):\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=50, line_search_fn='strong_wolfe')\n",
    "    # If there is a leave-out indices list, exclude these indies from the training set\n",
    "    if leave_out_index is not None:\n",
    "        train_data = torch.cat((train_data[:leave_out_index], train_data[leave_out_index + 1:]), dim=0) \n",
    "        train_labels = torch.cat((train_labels[:leave_out_index], train_labels[leave_out_index + 1:]), dim=0)        \n",
    "    \n",
    "    for epoch in range(t):  \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data)\n",
    "            loss = criterion(output, train_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        loss_val = optimizer.step(closure)\n",
    "        if writer:\n",
    "            writer.add_scalar('training loss', loss_val, epoch)\n",
    "        # print(f\"epoch {epoch} finished, loss={loss_val}\")        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Batch validation for the model\n",
    "'''\n",
    "def test(model, criterion, test_dataset):\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    incorrect_data_list, incorrect_label_list = [], []\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(data.view(-1, 28*28))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update validation metrics (e.g., accuracy)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            incorrect_data_list.append(data[predicted != labels])\n",
    "            incorrect_label_list.append(labels[predicted != labels])\n",
    "            \n",
    "    incorrect_data = torch.cat(incorrect_data_list, dim=0)\n",
    "    incorrect_label = torch.cat(incorrect_label_list, dim=0)\n",
    "    val_loss /= len(test_dataset)\n",
    "    val_accuracy = 100 * val_correct / len(test_dataset)\n",
    "    return val_loss, val_accuracy, incorrect_data, incorrect_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the baseline model in L-BFGS Optimizer and calculate $L(z, \\hat{\\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a summary writer for plotting training loss\n",
    "'''\n",
    "model_train_writer = SummaryWriter('runs/logistic_regression_10_mnist_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression(28*28, 10).to(device)\n",
    "# Store the initial parameter values for later leave-one-out retrain\n",
    "initial_params = {\"weight\": base_model.linear.weight.data.clone(), \"bias\": base_model.linear.bias.data.clone()}\n",
    "\n",
    "def criterion_l2(output, target, model):\n",
    "    loss = criterion(output, target)\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param, 2)**2\n",
    "    loss += damp * l2_reg\n",
    "    return loss * scale\n",
    "\n",
    "base_model_criterion = lambda x, y: criterion_l2(x, y, base_model)\n",
    "train_lbfgs(base_model, base_model_criterion, train_data, train_labels, writer=model_train_writer, t=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0003, Validation Accuracy: 77.37%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy, incorrect_data, incorrect_label = test(base_model, base_model_criterion, test_dataset)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss difference of leave-one-out retraining\n",
    "\n",
    "According to the paper, we arbitrarily picked a wrongly-classified test point $z_{test}=(x_{test}, y_{test})$ as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_index = torch.randint(0, len(incorrect_data), (1,)).item()\n",
    "x_test, y_test = incorrect_data[test_data_index], incorrect_label[test_data_index].view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_leave_out_loss_diff(model, initial_params, criterion_l2, test_data, test_label, leave_out_index=None, t=1):\n",
    "    # Clone initial parameters\n",
    "    params = {\"weight\": initial_params[\"weight\"].clone(), \"bias\": initial_params[\"bias\"].clone()}\n",
    "    \n",
    "    # Calculate L(z_{test}, \\theta) which model is trained with all training points\n",
    "    loss_z_test_with_z = criterion_l2(model(test_data), test_label, model)\n",
    "    \n",
    "    # Train leave-one-out model\n",
    "    retrained_model = LogisticRegression(28*28, 10, params=params).to(device)\n",
    "    retrained_model_criterion = lambda x, y: criterion_l2(x, y, retrained_model)\n",
    "    train_lbfgs(retrained_model, retrained_model_criterion, train_data, train_labels, t=t, leave_out_index=leave_out_index)\n",
    "    \n",
    "    # Calculate L(z_{test}, \\theta_{-z}) which model is trained without data point z\n",
    "    loss_z_test_without_z = retrained_model_criterion(retrained_model(test_data), test_label)\n",
    "    \n",
    "    leave_out_loss_diff = loss_z_test_without_z - loss_z_test_with_z\n",
    "    # print(f\"loss_z_test_with_z: {loss_z_test_with_z}, loss_z_test_without_z: {loss_z_test_without_z}, leave_out_loss_diff: {leave_out_loss_diff}\")\n",
    "    return leave_out_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Validate the functionality of calc_leave_out_loss_diff\n",
    "'''\n",
    "leave_out_loss_diff = calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=None)\n",
    "assert leave_out_loss_diff == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Influence Function\n",
    "\n",
    "### 0. Preparation\n",
    "\n",
    "Since in influence function stochastic estimation, we need to uniformly sample $t$ points from training set, an uniform sampling method would be neccessary to be defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample_dataset(dataset, t, batch_size=1):\n",
    "    sampler = torch.utils.data.sampler.RandomSampler(dataset, num_samples=t)\n",
    "    sampled_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return sampled_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method to calculate first order gradient of multivariable scalar function (in our case, loss function) for all the parameters:\n",
    "$$\\nabla_{\\theta} L(z, \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_first_order_gradient(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward()\n",
    "    param_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    param_grads = torch.cat(param_grads)\n",
    "    return param_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method to calculate second order gradient of multivariable scalar function (i.e. Hessian matrix) for all the parameters:\n",
    "$$\\nabla^{2}_{\\theta} L(z, \\theta)$$\n",
    "\n",
    "Noted this method would not be used for any reason other than validation purpose, since it is not efficient and for our implementation, HVP (Hessian Vector Product) would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_second_order_gradient(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = []\n",
    "    for first_grad in first_grads:\n",
    "        row = []\n",
    "        for p in model.parameters():\n",
    "            # Compute every possible combination of differentiation variables for the parameters\n",
    "            sub_matrix = []\n",
    "            for i in range(first_grad.shape[0]):\n",
    "                sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "            sub_matrix = torch.stack(sub_matrix)\n",
    "            row.append(sub_matrix)\n",
    "        row = torch.cat(row, dim=1)\n",
    "        second_grads.append(row)\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HVP (Hessian Vector Product) Calculation\n",
    "\n",
    "This method could accer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(loss, params, vector):\n",
    "    # Compute the first-order gradient\n",
    "    first_grads = torch.autograd.grad(loss, params, create_graph=True)\n",
    "\n",
    "    first_grads = [g.flatten() for g in first_grads]\n",
    "    first_grads = torch.cat(first_grads, dim=0)\n",
    "    \n",
    "    # Calculate the product between the first gradients and the vector\n",
    "    first_grad_vector_product = torch.sum(first_grads * vector)\n",
    "\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = torch.autograd.grad(first_grad_vector_product, params, create_graph=True)\n",
    "    second_grads = [g.flatten() for g in second_grads]\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation for HVP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected HVP: tensor([-16.0884,  32.1769,   4.0581,  -8.1161, -16.0884,   4.0581],\n",
      "       grad_fn=<MvBackward0>) \n",
      "Actual HVP: tensor([-16.0884,  32.1769,   4.0581,  -8.1161, -16.0884,   4.0581],\n",
      "       grad_fn=<CatBackward0>) \n",
      "Diff Sum: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/torch/csrc/autograd/engine.cpp:1182.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/var/folders/hv/98gq3v9n7tq0z5f1l_9wzblh0000gn/T/ipykernel_31409/1855020473.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  expected = torch.matmul(calc_second_order_gradient(test_criterion(test_model(data_tensor), label_tensor), test_model), vector.T)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Validate the correctness of HVP's implementation\n",
    "'''\n",
    "\n",
    "test_model = LogisticRegression(2, 2)\n",
    "test_criterion = torch.nn.MSELoss()\n",
    "\n",
    "data_tensor = torch.tensor([[1.0, -2.0]], requires_grad=True)\n",
    "label_tensor = torch.tensor([1.0, 0.0])\n",
    "\n",
    "params = [p for p in test_model.parameters()]\n",
    "vector = calc_first_order_gradient(test_criterion(test_model(data_tensor), label_tensor), test_model)\n",
    "expected = torch.matmul(calc_second_order_gradient(test_criterion(test_model(data_tensor), label_tensor), test_model), vector.T)\n",
    "vector._grad_fn = None\n",
    "test_model.zero_grad()\n",
    "actual = hvp(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), params, vector)\n",
    "print(f\"Expected HVP: {expected} \\nActual HVP: {actual} \\nDiff Sum: {(expected-actual).sum()}\")\n",
    "assert torch.equal(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. IHVP (Inverse Hessian Vector Product) Calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ihvp(train_dataset, test_data, test_label, model, criterion, t, r, unique_datapoint=None, ihvp_summary_writer=None):\n",
    "    ihvp_eval_avg = 0\n",
    "    vector = calc_first_order_gradient(criterion(model(test_data), test_label), model)\n",
    "    for i in range(r):\n",
    "        # Arg unique_datapoint is used for debugging. It would generate an assigned single-data dataset\n",
    "        if unique_datapoint is None:\n",
    "            sampled_train_dataset = [(data, label) for data, label in uniform_sample_dataset(train_dataset, t)]\n",
    "        else:\n",
    "            sampled_train_dataset = []\n",
    "            for _ in range(t):\n",
    "                sampled_train_dataset.append(unique_datapoint)\n",
    "        # Step 1. Initialize the evaluation of the Hessian-vector product\n",
    "        ihvp_eval = vector\n",
    "        data_number = 0\n",
    "        for data, label in sampled_train_dataset:\n",
    "            # data, label = data.to(device), label.to(device)\n",
    "            # Step 2. Compute the second order gradient of the loss w.r.t. the model parameters\n",
    "            model.zero_grad()\n",
    "            data_tensor = data.view(-1, 28*28)\n",
    "            params = [p for p in model.parameters()]\n",
    "            ihvp_eval._grad_fn = None\n",
    "            # Step 3. Compute the inner product between the Hessian matrix and the test gradient vector using HVP\n",
    "            hvp_eval = hvp(criterion(model(data_tensor), label), params, ihvp_eval)\n",
    "            ihvp_eval = ihvp_eval + vector - hvp_eval\n",
    "            if ihvp_summary_writer:\n",
    "                ihvp_summary_writer.add_scalar(f'ihvp_eval_sum_{i}', ihvp_eval.sum(), data_number)\n",
    "            data_number += 1\n",
    "        print(f\"ihvp iteration {i} done and ihvp sum is {ihvp_eval.sum()}\")\n",
    "        ihvp_eval_avg = i / (i + 1) * ihvp_eval_avg + 1 / (i + 1) * ihvp_eval   \n",
    "    return ihvp_eval_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation for IHVP**: \n",
    "Check if IHVP successfully convergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data_loader  = uniform_sample_dataset(train_dataset, 1)\n",
    "uni_data_tuple = ()\n",
    "for data, label in uni_data_loader:\n",
    "    uni_data_tuple = (data.to(device), label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data value sum: 84.62353515625, Test data label: tensor([3])\n",
      "ihvp iteration 0 done and ihvp sum is -0.00037457793951034546\n",
      "ihvp iteration 1 done and ihvp sum is -0.00037629902362823486\n",
      "ihvp iteration 2 done and ihvp sum is -0.00037566572427749634\n",
      "ihvp iteration 3 done and ihvp sum is -0.0003763362765312195\n",
      "ihvp iteration 4 done and ihvp sum is -0.0003770068287849426\n",
      "ihvp iteration 5 done and ihvp sum is -0.000376187264919281\n",
      "ihvp iteration 6 done and ihvp sum is -0.00037470459938049316\n",
      "ihvp iteration 7 done and ihvp sum is -0.00037657469511032104\n",
      "ihvp iteration 8 done and ihvp sum is -0.00037382543087005615\n",
      "ihvp iteration 9 done and ihvp sum is -0.00037570297718048096\n",
      "tensor([-5.7487e-06,  5.3363e-06,  3.3305e-07,  ...,  2.3679e-02,\n",
      "         1.3323e-02,  2.1043e-02], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ihvp_summary_writer = SummaryWriter('runs/ihvp_sum_summary') \n",
    "print(f\"Test data value sum: {x_test.sum()}, Test data label: {y_test}\")\n",
    "ihvp_eval = ihvp(train_dataset, x_test, y_test, base_model, base_model_criterion, 5000, 10, ihvp_summary_writer=ihvp_summary_writer)\n",
    "print(ihvp_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0004, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihvp_eval.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Influence Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upweighting_loss_influence_function(s_test, train_dataset, upweighted_data, upweighted_label, test_data, test_label, model, criterion):\n",
    "    # Step 1. Compute the Inverse Hessian-vector product\n",
    "    # ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion, 5000, 5)\n",
    "    # Step 2. Compute the influence function\n",
    "    first_grad = calc_first_order_gradient(criterion(model(upweighted_data), upweighted_label), model)\n",
    "    influence = torch.matmul(-s_test, first_grad)\n",
    "    return influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[0], train_labels[0].view(1), x_test, y_test, base_model, base_model_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.3143e-07, grad_fn=<DivBackward0>),\n",
       " tensor(-8.3447e-07, grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-influence / n, calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/55000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 55000/55000 [00:21<00:00, 2589.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "influence_list = []\n",
    "for i in tqdm(range(n), desc=\"Processing\"):\n",
    "    influence = upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[i], train_labels[i].view(1), x_test, y_test, base_model, base_model_criterion)\n",
    "    predicted_diff = (influence, -influence / n, i)\n",
    "    influence_list.append(predicted_diff)\n",
    "    \n",
    "top_abs_500_list = sorted(influence_list, key=lambda pair: torch.abs(pair[0]), reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_500_list = sorted(influence_list, key=lambda pair: torch.abs(pair[0]), reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_500_1000_list = sorted(influence_list, key=lambda pair: torch.abs(pair[0]), reverse=True)[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progressing...: 100%|██████████| 500/500 [1:28:12<00:00, 10.58s/it]\n"
     ]
    }
   ],
   "source": [
    "influence_leave_out_diff_list = []\n",
    "for influence, predict, idx in tqdm(top_abs_500_list, desc=\"Progressing...\"):\n",
    "    leave_out_loss_diff = calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=idx)\n",
    "    pair = (predict, leave_out_loss_diff)\n",
    "    influence_leave_out_diff_list.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progressing...: 100%|██████████| 500/500 [1:28:41<00:00, 10.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# influence_leave_out_diff_list_1000 = []\n",
    "for influence, predict, idx in tqdm(top_abs_500_1000_list, desc=\"Progressing...\"):\n",
    "    leave_out_loss_diff = calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=idx)\n",
    "    pair = (predict, leave_out_loss_diff)\n",
    "    influence_leave_out_diff_list.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_list, y_list = [], []\n",
    "for y, x in influence_leave_out_diff_list:\n",
    "    # if torch.abs(-x-y)>4*torch.abs(x):\n",
    "    #     continue\n",
    "    x_list.append(x.item())\n",
    "    y_list.append(y.item())\n",
    "    \n",
    "x_dig = [0, 5e-6]\n",
    "y_dig = [0, 5e-6]\n",
    "plt.scatter(x_list, y_list)\n",
    "plt.plot(x_dig, y_dig)\n",
    "\n",
    "print(len(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1499.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_influence_list = sorted(influence_list, key=lambda pair: torch.abs(pair[1]), reverse=True)[:3000]\n",
    "positive_count = 0\n",
    "positive_rank_avg = 0\n",
    "for i in range(len(sorted_influence_list)):\n",
    "    ele = sorted_influence_list[i]\n",
    "    if ele[1] > 0:\n",
    "        positive_rank_avg = positive_count / (positive_count + 1) * positive_rank_avg + 1 / (positive_count + 1) * i\n",
    "        positive_count += 1\n",
    "positive_count, positive_rank_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored = [influence_list, influence_leave_out_diff_list]\n",
    "torch.save(stored, \"influence_lists.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted = torch.load(\"influence_lists.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sorted(influence_list, key=lambda pair: pair[0], reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_500_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
