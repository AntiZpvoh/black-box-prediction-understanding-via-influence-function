{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(folder_path)\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist\")\n",
    "\n",
    "delete_folder('./runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-defined constants\n",
    "'''\n",
    "n = 55000  # The size of training set\n",
    "scale = 1e-2 # the scale factor for loss function\n",
    "damp = 1 # the damp factor to add L2 regularization in loss function\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function without scaling and regularization\n",
    "device = 'cpu'\n",
    "iteration = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, we would load training dataset and testing dataset from MNIST. We assign the training set size $n=55000$, which is accorded to the paper Understanding Black-box Predictions via Influence Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "train_dataset_all = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transform, )\n",
    "\n",
    "train_indices = torch.randperm(len(train_dataset_all))[:n]\n",
    "train_dataset = Subset(train_dataset_all, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_list = []\n",
    "all_labels_list = []\n",
    "for i in range(len(train_dataset)):\n",
    "    data, label = train_dataset[i]\n",
    "    all_data_list.append(data)         # img shape: [1, 28, 28]\n",
    "    all_labels_list.append(label)\n",
    "\n",
    "train_data = torch.stack(all_data_list, dim=0).to(device)  # shape [n, 1, 28, 28]\n",
    "train_labels = torch.tensor(all_labels_list).to(device)        # shape [n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Logistic Regression Model Definition\n",
    "\n",
    "Below we defined the logistic regression model with training and testing method using L-BFGS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, params=None):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if params:\n",
    "            self.linear.weight = nn.Parameter(params['weight'])\n",
    "            self.linear.bias = nn.Parameter(params['bias'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        # outputs = torch.nn.functional.softmax(self.linear(x))\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs(model, criterion, train_data, train_labels, writer=None, t=20, leave_out_index=None):\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=50, line_search_fn='strong_wolfe')\n",
    "    # If there is a leave-out indices list, exclude these indies from the training set\n",
    "    if leave_out_index is not None:\n",
    "        train_data = torch.cat((train_data[:leave_out_index], train_data[leave_out_index + 1:]), dim=0) \n",
    "        train_labels = torch.cat((train_labels[:leave_out_index], train_labels[leave_out_index + 1:]), dim=0)        \n",
    "    \n",
    "    for epoch in range(t):  \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data)\n",
    "            loss = criterion(output, train_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        loss_val = optimizer.step(closure)\n",
    "        if writer:\n",
    "            writer.add_scalar('training loss', loss_val, epoch)\n",
    "        # print(f\"epoch {epoch} finished, loss={loss_val}\")        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Batch validation for the model\n",
    "'''\n",
    "def test(model, criterion, test_dataset):\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    incorrect_data_list, incorrect_label_list = [], []\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(data.view(-1, 28*28))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update validation metrics (e.g., accuracy)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            incorrect_data_list.append(data[predicted != labels])\n",
    "            incorrect_label_list.append(labels[predicted != labels])\n",
    "            \n",
    "    incorrect_data = torch.cat(incorrect_data_list, dim=0)\n",
    "    incorrect_label = torch.cat(incorrect_label_list, dim=0)\n",
    "    val_loss /= len(test_dataset)\n",
    "    val_accuracy = 100 * val_correct / len(test_dataset)\n",
    "    return val_loss, val_accuracy, incorrect_data, incorrect_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the baseline model in L-BFGS Optimizer and calculate $L(z, \\hat{\\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a summary writer for plotting training loss\n",
    "'''\n",
    "model_train_writer = SummaryWriter('runs/logistic_regression_10_mnist_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression(28*28, 10).to(device)\n",
    "# Store the initial parameter values for later leave-one-out retrain\n",
    "initial_params = {\"weight\": base_model.linear.weight.data.clone(), \"bias\": base_model.linear.bias.data.clone()}\n",
    "\n",
    "def criterion_l2(output, target, model):\n",
    "    loss = criterion(output, target)\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param, 2)**2\n",
    "    loss += damp * l2_reg\n",
    "    return loss * scale\n",
    "\n",
    "base_model_criterion = lambda x, y: criterion_l2(x, y, base_model)\n",
    "train_lbfgs(base_model, base_model_criterion, train_data, train_labels, writer=model_train_writer, t=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0003, Validation Accuracy: 81.84%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy, incorrect_data, incorrect_label = test(base_model, base_model_criterion, test_dataset)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss difference of leave-one-out retraining\n",
    "\n",
    "According to the paper, we arbitrarily picked a wrongly-classified test point $z_{test}=(x_{test}, y_{test})$ as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_index = torch.randint(0, len(incorrect_data), (1,)).item()\n",
    "x_test, y_test = incorrect_data[test_data_index], incorrect_label[test_data_index].view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_leave_out_loss_diff(model, initial_params, criterion_l2, test_data, test_label, leave_out_index=None, t=1):\n",
    "    # Clone initial parameters\n",
    "    params = {\"weight\": initial_params[\"weight\"].clone(), \"bias\": initial_params[\"bias\"].clone()}\n",
    "    \n",
    "    # Calculate L(z_{test}, \\theta) which model is trained with all training points\n",
    "    loss_z_test_with_z = criterion_l2(model(test_data), test_label, model)\n",
    "    \n",
    "    # Train leave-one-out model\n",
    "    retrained_model = LogisticRegression(28*28, 10, params=params).to(device)\n",
    "    retrained_model_criterion = lambda x, y: criterion_l2(x, y, retrained_model)\n",
    "    train_lbfgs(retrained_model, retrained_model_criterion, train_data, train_labels, t=t, leave_out_index=leave_out_index)\n",
    "    \n",
    "    # Calculate L(z_{test}, \\theta_{-z}) which model is trained without data point z\n",
    "    loss_z_test_without_z = retrained_model_criterion(retrained_model(test_data), test_label)\n",
    "    \n",
    "    leave_out_loss_diff = loss_z_test_without_z - loss_z_test_with_z\n",
    "    # print(f\"loss_z_test_with_z: {loss_z_test_with_z}, loss_z_test_without_z: {loss_z_test_without_z}, leave_out_loss_diff: {leave_out_loss_diff}\")\n",
    "    return leave_out_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Validate the functionality of calc_leave_out_loss_diff\n",
    "'''\n",
    "leave_out_loss_diff = calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=None)\n",
    "assert leave_out_loss_diff == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Influence Function\n",
    "\n",
    "### 0. Preparation\n",
    "\n",
    "Since in influence function stochastic estimation, we need to uniformly sample $t$ points from training set, an uniform sampling method would be neccessary to be defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample_dataset(dataset, t, batch_size=1):\n",
    "    sampler = torch.utils.data.sampler.RandomSampler(dataset, num_samples=t)\n",
    "    sampled_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return sampled_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method to calculate first order gradient of multivariable scalar function (in our case, loss function) for all the parameters:\n",
    "$$\\nabla_{\\theta} L(z, \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_first_order_gradient(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    param_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    param_grads = torch.cat(param_grads)\n",
    "    return param_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method to calculate second order gradient of multivariable scalar function (i.e. Hessian matrix) for all the parameters:\n",
    "$$\\nabla^{2}_{\\theta} L(z, \\theta)$$\n",
    "\n",
    "Noted this method would not be used for any reason other than validation purpose, since it is not efficient and for our implementation, HVP (Hessian Vector Product) would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_second_order_gradient(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = []\n",
    "    for first_grad in first_grads:\n",
    "        row = []\n",
    "        for p in model.parameters():\n",
    "            # Compute every possible combination of differentiation variables for the parameters\n",
    "            sub_matrix = []\n",
    "            for i in range(first_grad.shape[0]):\n",
    "                sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "            sub_matrix = torch.stack(sub_matrix)\n",
    "            row.append(sub_matrix)\n",
    "        row = torch.cat(row, dim=1)\n",
    "        second_grads.append(row)\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HVP (Hessian Vector Product) Calculation\n",
    "\n",
    "This method could accer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(loss, params, vector):\n",
    "    # Compute the first-order gradient\n",
    "    first_grads = torch.autograd.grad(loss, params, create_graph=True)\n",
    "\n",
    "    first_grads = [g.flatten() for g in first_grads]\n",
    "    first_grads = torch.cat(first_grads, dim=0)\n",
    "    \n",
    "    # Calculate the product between the first gradients and the vector\n",
    "    first_grad_vector_product = torch.sum(first_grads * vector)\n",
    "\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = torch.autograd.grad(first_grad_vector_product, params, create_graph=True)\n",
    "    second_grads = [g.flatten() for g in second_grads]\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation for HVP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected HVP: tensor([-1.1512,  2.3024, -3.3856,  6.7712, -1.1512, -3.3856],\n",
      "       grad_fn=<MvBackward0>) \n",
      "Actual HVP: tensor([-1.1512,  2.3024, -3.3856,  6.7712, -1.1512, -3.3856],\n",
      "       grad_fn=<CatBackward0>) \n",
      "Diff Sum: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Validate the correctness of HVP's implementation\n",
    "'''\n",
    "\n",
    "test_model = LogisticRegression(2, 2)\n",
    "test_criterion = torch.nn.MSELoss()\n",
    "\n",
    "data_tensor = torch.tensor([[1.0, -2.0]], requires_grad=True)\n",
    "label_tensor = torch.tensor([1.0, 0.0])\n",
    "\n",
    "params = [p for p in test_model.parameters()]\n",
    "vector = calc_first_order_gradient(test_criterion(test_model(data_tensor), label_tensor), test_model)\n",
    "expected = torch.matmul(calc_second_order_gradient(test_criterion(test_model(data_tensor), label_tensor), test_model), vector.T)\n",
    "vector._grad_fn = None\n",
    "test_model.zero_grad()\n",
    "actual = hvp(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), params, vector)\n",
    "print(f\"Expected HVP: {expected} \\nActual HVP: {actual} \\nDiff Sum: {(expected-actual).sum()}\")\n",
    "assert torch.equal(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. IHVP (Inverse Hessian Vector Product) Calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ihvp(train_dataset, test_data, test_label, model, criterion, t, r, unique_datapoint=None, ihvp_summary_writer=None):\n",
    "    ihvp_eval_avg = 0\n",
    "    vector = calc_first_order_gradient(criterion(model(test_data), test_label), model)\n",
    "    for i in range(r):\n",
    "        # Arg unique_datapoint is used for debugging. It would generate an assigned single-data dataset\n",
    "        if unique_datapoint is None:\n",
    "            sampled_train_dataset = [(data, label) for data, label in uniform_sample_dataset(train_dataset, t)]\n",
    "        else:\n",
    "            sampled_train_dataset = []\n",
    "            for _ in range(t):\n",
    "                sampled_train_dataset.append(unique_datapoint)\n",
    "        # Step 1. Initialize the evaluation of the Hessian-vector product\n",
    "        ihvp_eval = vector\n",
    "        data_number = 0\n",
    "        for data, label in sampled_train_dataset:\n",
    "            # data, label = data.to(device), label.to(device)\n",
    "            # Step 2. Compute the second order gradient of the loss w.r.t. the model parameters\n",
    "            model.zero_grad()\n",
    "            data_tensor = data.view(-1, 28*28)\n",
    "            params = [p for p in model.parameters()]\n",
    "            ihvp_eval._grad_fn = None\n",
    "            # Step 3. Compute the inner product between the Hessian matrix and the test gradient vector using HVP\n",
    "            hvp_eval = hvp(criterion(model(data_tensor), label), params, ihvp_eval)\n",
    "            ihvp_eval = ihvp_eval + vector - hvp_eval\n",
    "            if ihvp_summary_writer:\n",
    "                ihvp_summary_writer.add_scalar(f'ihvp_eval_sum_{i}', ihvp_eval.sum(), data_number)\n",
    "            data_number += 1\n",
    "        print(f\"ihvp iteration {i} done and ihvp sum is {ihvp_eval.sum()}\")\n",
    "        ihvp_eval_avg = i / (i + 1) * ihvp_eval_avg + 1 / (i + 1) * ihvp_eval   \n",
    "    return ihvp_eval_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation for IHVP**: \n",
    "Check if IHVP successfully convergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data_loader  = uniform_sample_dataset(train_dataset, 1)\n",
    "uni_data_tuple = ()\n",
    "for data, label in uni_data_loader:\n",
    "    uni_data_tuple = (data.to(device), label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data value sum: -445.4039001464844, Test data label: tensor([8])\n",
      "ihvp iteration 0 done and ihvp sum is 0.0002600327134132385\n",
      "ihvp iteration 1 done and ihvp sum is 0.0002622567117214203\n",
      "ihvp iteration 2 done and ihvp sum is 0.0002576969563961029\n",
      "ihvp iteration 3 done and ihvp sum is 0.0002580508589744568\n",
      "ihvp iteration 4 done and ihvp sum is 0.00026191025972366333\n",
      "ihvp iteration 5 done and ihvp sum is 0.0002533644437789917\n",
      "ihvp iteration 6 done and ihvp sum is 0.00025463104248046875\n",
      "ihvp iteration 7 done and ihvp sum is 0.0002626851201057434\n",
      "ihvp iteration 8 done and ihvp sum is 0.0002617351710796356\n",
      "ihvp iteration 9 done and ihvp sum is 0.00025508925318717957\n",
      "tensor([-0.0057, -0.0057, -0.0057,  ...,  0.0027, -0.0315,  0.0033],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ihvp_summary_writer = SummaryWriter('runs/ihvp_sum_summary') \n",
    "print(f\"Test data value sum: {x_test.sum()}, Test data label: {y_test}\")\n",
    "ihvp_eval = ihvp(train_dataset, x_test, y_test, base_model, base_model_criterion, 5000, 10, ihvp_summary_writer=ihvp_summary_writer)\n",
    "print(ihvp_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0003, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihvp_eval.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Influence Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upweighting_loss_influence_function(s_test, train_dataset, upweighted_data, upweighted_label, test_data, test_label, model, criterion):\n",
    "    # Step 1. Compute the Inverse Hessian-vector product\n",
    "    # ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion, 5000, 5)\n",
    "    # Step 2. Compute the influence function\n",
    "    first_grad = calc_first_order_gradient(criterion(model(upweighted_data), upweighted_label), model)\n",
    "    influence = torch.matmul(-s_test, first_grad)\n",
    "    return influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[0], train_labels[0].view(1), x_test, y_test, base_model, base_model_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.3606e-07, grad_fn=<DivBackward0>),\n",
       " tensor(-3.7998e-07, grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-influence / n, calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_list = []\n",
    "for i in range(n):\n",
    "    influence = upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[i], train_labels[i].view(1), x_test, y_test, base_model, base_model_criterion)\n",
    "    predicted_diff = (torch.abs(influence), -influence / n, i)\n",
    "    influence_list.append(predicted_diff)\n",
    "    \n",
    "top_500_list = sorted(influence_list, key=lambda pair: pair[0], reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor(2.9538e-07, device='cuda:0', grad_fn=<DivBackward0>), tensor(-4.0606e-06, device='cuda:0', grad_fn=<SubBackward0>))\n",
      "index 0 saved\n",
      "1 (tensor(-2.1500e-08, device='cuda:0', grad_fn=<DivBackward0>), tensor(-1.3039e-06, device='cuda:0', grad_fn=<SubBackward0>))\n",
      "2 (tensor(-3.4369e-08, device='cuda:0', grad_fn=<DivBackward0>), tensor(-1.4771e-06, device='cuda:0', grad_fn=<SubBackward0>))\n",
      "3 (tensor(2.2645e-08, device='cuda:0', grad_fn=<DivBackward0>), tensor(-2.2966e-06, device='cuda:0', grad_fn=<SubBackward0>))\n",
      "4 (tensor(-5.2010e-08, device='cuda:0', grad_fn=<DivBackward0>), tensor(-9.2387e-07, device='cuda:0', grad_fn=<SubBackward0>))\n",
      "5 (tensor(-3.2962e-08, device='cuda:0', grad_fn=<DivBackward0>), tensor(-4.8503e-06, device='cuda:0', grad_fn=<SubBackward0>))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m55000\u001b[39m):\n\u001b[1;32m      3\u001b[0m     influence \u001b[38;5;241m=\u001b[39m upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[i], train_labels[i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m), x_test, y_test, base_model, base_model_criterion)\n\u001b[0;32m----> 4\u001b[0m     leave_out_loss_diff \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_leave_out_loss_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave_out_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     pair \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39minfluence \u001b[38;5;241m/\u001b[39m n, leave_out_loss_diff)\n\u001b[1;32m      6\u001b[0m     influence_leave_out_diff_list\u001b[38;5;241m.\u001b[39mappend(pair)\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mcalc_leave_out_loss_diff\u001b[0;34m(model, initial_params, criterion_l2, test_data, test_label, leave_out_index, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m retrained_model \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m10\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m retrained_model_criterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: criterion_l2(x, y, retrained_model)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_lbfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrained_model_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave_out_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleave_out_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate L(z_{test}, \\theta_{-z}) which model is trained without data point z\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss_z_test_without_z \u001b[38;5;241m=\u001b[39m retrained_model_criterion(retrained_model(test_data), test_label)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_lbfgs\u001b[0;34m(model, criterion, train_data, train_labels, writer, t, leave_out_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 16\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer:\n\u001b[1;32m     18\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_val, epoch)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/lbfgs.py:444\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 444\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    448\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/lbfgs.py:92\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     90\u001b[0m g_prev \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     91\u001b[0m gtd_prev \u001b[38;5;241m=\u001b[39m gtd_new\n\u001b[0;32m---> 92\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m ls_func_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/lbfgs.py:442\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/optim/lbfgs.py:296\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 296\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# influence_leave_out_diff_list = []\n",
    "# for i in range(55000):\n",
    "#     influence = upweighting_loss_influence_function(ihvp_eval, train_dataset, train_data[i], train_labels[i].view(1), x_test, y_test, base_model, base_model_criterion)\n",
    "#     leave_out_loss_diff = calc_leave_out_loss_diff(base_model, initial_params, criterion_l2, x_test, y_test, t=iteration, leave_out_index=i)\n",
    "#     pair = (-influence / n, leave_out_loss_diff)\n",
    "#     influence_leave_out_diff_list.append(pair)\n",
    "#     print(i, pair)\n",
    "#     if i % 100 == 0:\n",
    "#         torch.save(influence_leave_out_diff_list, 'influence_leave_out_diff_list.pt')\n",
    "#         print(f\"index {i} saved\")\n",
    "    \n",
    "# top_500_list = sorted(influence_leave_out_diff_list, key=lambda pair: pair[0], reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_abs_500_list = sorted(influence_leave_out_diff_list, key=lambda pair: torch.abs(pair[0]), reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"top_500.csv\", \"w\") as file:\n",
    "    file.write(\"x,y\\n\")\n",
    "    for x, y in top_500_list:\n",
    "        file.write(f\"{-x.item()},{y.item()}\\n\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"top_abs_500.csv\", \"w\") as file:\n",
    "    file.write(\"x,y\\n\")\n",
    "    for x, y in top_abs_500_list:\n",
    "        file.write(f\"{-x.item()},{y.item()}\\n\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_list, y_list = [], []\n",
    "for x, y in top_abs_500_list:\n",
    "    # if torch.abs(-x-y)>4*torch.abs(x):\n",
    "    #     continue\n",
    "    x_list.append(x.item())\n",
    "    y_list.append(y.item())\n",
    "    \n",
    "x_dig = [-6e-6, 6e-6]\n",
    "y_dig = [-6e-6, 6e-6]\n",
    "plt.scatter(x_list, y_list)\n",
    "plt.plot(x_dig, y_dig)\n",
    "\n",
    "print(len(x_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
