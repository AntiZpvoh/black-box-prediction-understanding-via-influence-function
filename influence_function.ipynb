{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(folder_path)\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist\")\n",
    "\n",
    "delete_folder('./runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transforms.ToTensor(), )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim - 1)\n",
    "        # self.linear.weight = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        # self.linear.bias = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        # outputs = torch.nn.functional.softmax(self.linear(x))\n",
    "        outputs = self.linear(x)\n",
    "        zeros_for_last_class = torch.zeros(\n",
    "            (outputs.shape[0], 1),\n",
    "            device=x.device,\n",
    "            dtype=x.dtype\n",
    "        )\n",
    "        output_with_zeros = torch.cat((outputs, zeros_for_last_class), dim=1)\n",
    "        return output_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/logistic_regression_10_mnist') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(28*28, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_num = 0\n",
    "for epoch in range(5):\n",
    "    model.train() # Set the model to training mode\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = batch_data.view(-1, 28*28)\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        writer.add_scalar('training loss', loss, batch_num)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "val_loss = 0\n",
    "val_correct = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for data, labels in test_loader:\n",
    "        # Transfer data to the appropriate device (CPU or GPU)\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data.view(-1, 28*28))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Update validation metrics (e.g., accuracy)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        val_correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2777, Validation Accuracy: 92.26%\n"
     ]
    }
   ],
   "source": [
    "val_loss /= len(test_loader)\n",
    "val_accuracy = 100 * val_correct / len(test_dataset)\n",
    "\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample_dataset(dataset, t):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    sampler = torch.utils.data.sampler.RandomSampler(dataset, num_samples=t)\n",
    "    sampled_dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, sampler=sampler)\n",
    "    return sampled_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28]) tensor([3, 8, 1, 8, 8, 6, 0, 3, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "sample_loader = uniform_sample_dataset(train_dataset, 10)\n",
    "for batch_data, batch_labels in sample_loader:\n",
    "    print(batch_data.shape, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_first_order_derivative(loss, model):\n",
    "    # Set requires_grad to True for the data tensor to enable gradient computation\n",
    "    # data_tensor.requires_grad = True\n",
    "    # label_tensor.requires_grad = True\n",
    "    model.zero_grad()\n",
    "    # output = model(data_tensor)\n",
    "    # loss = criterion(output, label_tensor)\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    param_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    param_grads = torch.cat(param_grads)\n",
    "    # print(param_grads)\n",
    "    # return torch.concat([data_tensor.grad.reshape(1, -1), label_tensor.grad.reshape(1, -1)], dim=1)\n",
    "    return param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_second_order_derivative_list(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ (p.grad.flatten(), p) for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = []\n",
    "    for first_grad, p in first_grads:\n",
    "        sub_matrix = []\n",
    "        # print(first_grad.shape, p.shape)\n",
    "        for i in range(first_grad.shape[0]):\n",
    "            sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "        sub_matrix = torch.stack(sub_matrix)\n",
    "        second_grads.append(sub_matrix)\n",
    "    # second_order_grads = torch.cat(second_order_grads)\n",
    "    print(second_grads)\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_second_order_derivative(loss, model):\n",
    "    # Set requires_grad to True for the data tensor to enable gradient computation\n",
    "    # data_tensor.requires_grad = True\n",
    "    # label_tensor.requires_grad = True\n",
    "    # loss = criterion(output, label_tensor)\n",
    "    model.zero_grad()\n",
    "    # output = model(data_tensor)\n",
    "    # loss = criterion(output, label_tensor)\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    # first_order_derivative.requires_grad = True\n",
    "    second_grads = []\n",
    "    for first_grad in first_grads:\n",
    "        row = []\n",
    "        for p in model.parameters():\n",
    "            sub_matrix = []\n",
    "            # print(first_grad.shape, p.shape)\n",
    "            for i in range(first_grad.shape[0]):\n",
    "                sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "            sub_matrix = torch.stack(sub_matrix)\n",
    "            # print(sub_matrix)\n",
    "            row.append(sub_matrix)\n",
    "        row = torch.cat(row, dim=1)\n",
    "        # print(row)\n",
    "        second_grads.append(row)\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "    # print(\"sec: \", second_grads)\n",
    "    # hessian_matrix = torch.autograd.functional.hessian(lambda x: criterion(model(x), label_tensor), data_tensor).reshape(data_tensor.shape[1], -1)\n",
    "    # hessian_matrix = torch.autograd.functional.hessian(lambda x, y: criterion(model(x), y), (data_tensor, label_tensor))\n",
    "    return second_grads\n",
    "    # matrix_list = []\n",
    "    # for row, var in zip(hessian_matrix, [data_tensor, label_tensor]):\n",
    "    #     list_row = []\n",
    "    #     for tensor in row:\n",
    "    #         if len(var.shape) == 1:\n",
    "    #             list_row.append(tensor.reshape(1, -1))\n",
    "    #         else:\n",
    "    #             list_row.append(tensor.reshape(var.shape[1], -1))\n",
    "    #     matrix_list.append(list_row)\n",
    "    # # Concatenate along the first dimension\n",
    "    # concatenated_hessian = torch.cat([torch.cat(row, dim=1) for row in matrix_list], dim=0)\n",
    "\n",
    "    # return concatenated_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.nn.Linear(2, 2)\n",
    "test_model = LogisticRegression(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0851, -0.5285]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data_tensor = torch.tensor([[1.0, -2.0]], requires_grad=True)\n",
    "label_tensor = torch.tensor([1.0, 0.0])\n",
    "label_tensor_long = torch.tensor([1, 0])\n",
    "\n",
    "print(test_model(data_tensor)-label_tensor)\n",
    "# 2 * (test_model(data_tensor) - label_tensor).item() * data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0851,  0.1702, -0.5285,  1.0570, -0.0851, -0.5285],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/torch/csrc/autograd/engine.cpp:1182.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(calc_criterion_first_order_derivative(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()(test_model(data_tensor), label_tensor), test_model))\n\u001b[0;32m----> 2\u001b[0m hessian \u001b[38;5;241m=\u001b[39m calc_criterion_second_order_derivative(\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_tensor_long\u001b[49m\u001b[43m)\u001b[49m, test_model)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(hessian, torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvals(hessian)\u001b[38;5;241m.\u001b[39mreal \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      4\u001b[0m calc_loss_second_order_derivative_list(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()(test_model(data_tensor), label_tensor), test_model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "print(calc_criterion_first_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model))\n",
    "hessian = calc_criterion_second_order_derivative(criterion(test_model(data_tensor), label_tensor_long), test_model)\n",
    "print(hessian, torch.all(torch.linalg.eigvals(hessian).real >= 0))\n",
    "calc_loss_second_order_derivative_list(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7850])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "one_train_dataloader = uniform_sample_dataset(train_dataset, 1)\n",
    "for batch_data, batch_labels in one_train_dataloader:\n",
    "    print(calc_criterion_first_order_derivative(criterion(model(batch_data), batch_labels), model).shape)\n",
    "    hessian = calc_criterion_second_order_derivative(torch.nn.MSELoss()(model(batch_data), batch_labels.to(torch.float32)), model)\n",
    "    print(hessian.shape, torch.all(torch.linalg.eigvals(hessian).real >= 0))\n",
    "    # hessian = calc_loss_second_order_derivative_list(criterion(model(batch_data), batch_labels), model)\n",
    "    # hessian_1 = hessian[0]\n",
    "    # hessian_2 = hessian[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = torch.linalg.eigvals(hessian).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(y, w, v):\n",
    "    # First derivative\n",
    "    first_grads = torch.autograd.grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    first_grads = [g.flatten() for g in first_grads]\n",
    "    first_grads = torch.cat(first_grads, dim=0)\n",
    "    \n",
    "    # Calculate the element-wise product between the first gradients and the vector v\n",
    "    elemwise_products = torch.sum(first_grads * v)\n",
    "\n",
    "    # Second derivative\n",
    "    second_grads = torch.autograd.grad(elemwise_products, w, create_graph=True)\n",
    "    second_grads = [g.flatten() for g in second_grads]\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0851,  0.1702, -0.5285,  1.0570, -0.0851, -0.5285],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([-0.5105,  1.0211, -3.1710,  6.3419, -0.5105, -3.1710],\n",
      "       grad_fn=<MvBackward0>) tensor([-0.5105,  1.0211, -3.1710,  6.3419, -0.5105, -3.1710],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/98gq3v9n7tq0z5f1l_9wzblh0000gn/T/ipykernel_15388/3960221844.py:5: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  expected = torch.matmul(calc_criterion_second_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model), vector.T)\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in test_model.parameters()]\n",
    "vector = calc_criterion_first_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model)\n",
    "print(vector)\n",
    "# vector = torch.tensor([-3.0305, -6.0611, -3.0305])\n",
    "expected = torch.matmul(calc_criterion_second_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model), vector.T)\n",
    "vector._grad_fn = None\n",
    "# print(vector)\n",
    "test_model.zero_grad()\n",
    "actual = hvp(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), params, vector)\n",
    "print(expected, actual)\n",
    "assert torch.equal(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ihvp(train_dataset, test_data, test_label, model, criterion, t, r, reg_factor=0.91, scale=0.04, unique_datapoint=None, ihvp_summary_writer=SummaryWriter('runs/ihvp_sum_summary')):\n",
    "    ihvp_eval_avg = 0\n",
    "    test_data = test_data\n",
    "    def scaled_criterion(output, label):\n",
    "        return criterion(output, label) * scale\n",
    "    \n",
    "    vector = calc_criterion_first_order_derivative(scaled_criterion(model(test_data), test_label), model)\n",
    "    for i in range(r):\n",
    "        if unique_datapoint is None:\n",
    "            sampled_train_dataset = [(data, label) for data, label in uniform_sample_dataset(train_dataset, t)]\n",
    "        else:\n",
    "            sampled_train_dataset = []\n",
    "            for _ in range(t):\n",
    "                sampled_train_dataset.append(unique_datapoint)\n",
    "        # Step 1. Initialize the evaluation of the Hessian-vector product\n",
    "        ihvp_eval = vector\n",
    "        data_number = 0\n",
    "        for data, label in sampled_train_dataset:\n",
    "            # Step 2. Compute the second order gradient of the loss w.r.t. the model parameters\n",
    "            model.zero_grad()\n",
    "            data_tensor = data.view(-1, 28*28)\n",
    "            params = [p for p in model.parameters()]\n",
    "            ihvp_eval._grad_fn = None\n",
    "            # Step 3. Compute the inner product between the Hessian matrix and the test gradient vector using HVP\n",
    "            second_grads = hvp(scaled_criterion(model(data_tensor), label), params, ihvp_eval)\n",
    "            # hessian = calc_criterion_second_order_derivative(scaled_criterion(model(data_tensor), label), model)\n",
    "            # hessian_list = calc_loss_second_order_derivative_list(scaled_criterion(model(data_tensor), label), model)\n",
    "            # diff = torch.norm(torch.eye(hessian.shape[0])-hessian, p=2)\n",
    "            # print(f\"is semi positive definite: {torch.all(torch.linalg.eigvals(hessian).real >= 0)}, is semi positive definite list: {[torch.all(torch.linalg.eigvals(h).real >= 0) for h in hessian_list]}\")\n",
    "            # print(f\"diff: {diff}, l2 for I: {torch.norm(torch.eye(hessian.shape[0]), p=2)}, hes: {hessian.sum()}, eig: {torch.max(torch.abs(torch.linalg.eigvals(hessian)))}\")\n",
    "            # return_grads_validation = torch.matmul(hessian, ihvp_eval.T)\n",
    "            # print(f\"diff: {torch.abs(return_grads.sum() - vector.sum())}, ihvp_eval: {ihvp_eval.sum()}, return_grads: {return_grads.sum()}, vector: {vector.sum()}\")\n",
    "            # print(f\"validation: {return_grads_validation.sum()}, hessian: {hessian.sum()}\")\n",
    "            ihvp_eval = (1 - reg_factor) * ihvp_eval + vector - second_grads\n",
    "            ihvp_summary_writer.add_scalar(f'ihvp_eval_sum_{i}', ihvp_eval.sum(), data_number)\n",
    "            data_number += 1\n",
    "        # print(f\"ihvp iteration {i} done and ihvp sum is {ihvp_eval.sum()}\")\n",
    "        ihvp_eval_avg = i / (i + 1) * ihvp_eval_avg + 1 / (i + 1) * ihvp_eval         \n",
    "    return ihvp_eval_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = uniform_sample_dataset(test_dataset, 1)\n",
    "test_data_list = [(data, label) for data, label in test_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data_loader  = uniform_sample_dataset(train_dataset, 1)\n",
    "uni_data_tuple = ()\n",
    "for data, label in uni_data_loader:\n",
    "    uni_data_tuple = (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data value sum: 73.66667175292969, Test data label: tensor([4])\n"
     ]
    }
   ],
   "source": [
    "ihvp_summary_writer = SummaryWriter('runs/ihvp_sum_summary') \n",
    "for test_data, test_label in test_data_list:\n",
    "    print(f\"Test data value sum: {test_data.sum()}, Test data label: {test_label}\")\n",
    "    model.zero_grad()\n",
    "    ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion, 5000, 10, reg_factor=0.96, scale=0.09, ihvp_summary_writer=ihvp_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0505e-06, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihvp_eval.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upweighting_loss_influence_function(train_dataset, upweighted_data, upweighted_label, test_data, test_label, model, criterion):\n",
    "    # Step 1. Compute the Hessian-vector product\n",
    "    ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion)\n",
    "    # Step 2. Compute the influence function\n",
    "    influence = torch.dot(-ihvp_eval, calc_criterion_first_order_derivative(upweighted_data, upweighted_label, criterion, model))\n",
    "    return influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0016227766016836\n",
      "1.0032481866072664\n",
      "1.0048762342901745\n",
      "1.0065069239307687\n",
      "1.0081402598163562\n",
      "1.0097762462412017\n",
      "1.011414887506538\n",
      "1.0130561879205784\n",
      "1.0147001517985268\n",
      "1.0163467834625903\n",
      "1.0179960872419902\n",
      "1.019648067472972\n",
      "1.021302728498819\n",
      "1.022960074669863\n",
      "1.024620110343494\n",
      "1.0262828398841741\n",
      "1.0279482676634477\n",
      "1.0296163980599533\n",
      "1.031287235459435\n",
      "1.032960784254754\n",
      "1.0346370488458994\n",
      "1.0363160336400015\n",
      "1.0379977430513423\n",
      "1.0396821815013666\n",
      "1.0413693534186945\n",
      "1.043059263239133\n",
      "1.044751915405687\n",
      "1.0464473143685715\n",
      "1.0481454645852237\n",
      "1.0498463705203136\n",
      "1.0515500366457566\n",
      "1.0532564674407252\n",
      "1.0549656673916599\n",
      "1.0566776409922831\n",
      "1.0583923927436076\n",
      "1.0601099271539522\n",
      "1.0618302487389504\n",
      "1.0635533620215638\n",
      "1.0652792715320947\n",
      "1.067007981808196\n",
      "1.068739497394884\n",
      "1.070473822844552\n",
      "1.072210962716979\n",
      "1.0739509215793448\n",
      "1.0756937040062404\n",
      "1.0774393145796801\n",
      "1.0791877578891147\n",
      "1.0809390385314404\n",
      "1.082693161111016\n",
      "1.08445013023967\n",
      "1.086209950536716\n",
      "1.0879726266289629\n",
      "1.0897381631507286\n",
      "1.0915065647438518\n",
      "1.0932778360577022\n",
      "1.0950519817491962\n",
      "1.0968290064828063\n",
      "1.0986089149305749\n",
      "1.1003917117721251\n",
      "1.1021774016946757\n",
      "1.1039659893930507\n",
      "1.1057574795696923\n",
      "1.1075518769346748\n",
      "1.1093491862057152\n",
      "1.1111494121081869\n",
      "1.112952559375131\n",
      "1.114758632747269\n",
      "1.1165676369730162\n",
      "1.1183795768084934\n",
      "1.120194457017539\n",
      "1.1220122823717231\n",
      "1.123833057650358\n",
      "1.1256567876405115\n",
      "1.1274834771370212\n",
      "1.1293131309425042\n",
      "1.131145753867372\n",
      "1.1329813507298419\n",
      "1.1348199263559504\n",
      "1.1366614855795654\n",
      "1.1385060332423989\n",
      "1.1403535741940205\n",
      "1.1422041132918692\n",
      "1.1440576554012665\n",
      "1.1459142053954288\n",
      "1.1477737681554814\n",
      "1.1496363485704701\n",
      "1.151501951537376\n",
      "1.1533705819611242\n",
      "1.1552422447546011\n",
      "1.1571169448386656\n",
      "1.1589946871421613\n",
      "1.1608754766019314\n",
      "1.1627593181628295\n",
      "1.1646462167777338\n",
      "1.1665361774075604\n",
      "1.1684292050212752\n",
      "1.1703253045959077\n",
      "1.1722244811165643\n",
      "1.174126739576441\n",
      "1.176032084976837\n",
      "1.1779405223271666\n",
      "[[-400.  300.]\n",
      " [ 300. -200.]]\n",
      "[[ 77.70247895 -44.32744073]\n",
      " [-44.32744073  48.1508518 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "I = np.identity(2)\n",
    "A = np.array([[2e-2,3e-2],[3e-2,4e-2]])\n",
    "\n",
    "print(np.linalg.norm(I - A, ord=2))\n",
    "A_inv = I\n",
    "\n",
    "A_inv_ = np.linalg.inv(A)\n",
    "\n",
    "for i in range(100):\n",
    "    A_inv = A_inv + I - A@A_inv\n",
    "    print(np.linalg.norm(I - A@A_inv, ord=2))\n",
    "    \n",
    "print(A_inv_)\n",
    "print(A_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_semi_positive_definite(matrix):\n",
    "    return torch.all(torch.linalg.eigvals(matrix).real >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 2., 3.]], requires_grad=True) Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([[13.,  0.]], grad_fn=<CatBackward0>) tensor([0])\n",
      "hessain expected: tensor([[6.1442e-06, 1.2288e-05, 1.8432e-05],\n",
      "        [1.2288e-05, 2.4577e-05, 3.6865e-05],\n",
      "        [1.8432e-05, 3.6865e-05, 5.5297e-05]], grad_fn=<MulBackward0>), is semi positive definite: False\n",
      "[tensor([[2.2650e-06, 4.5300e-06, 6.7949e-06],\n",
      "        [4.5300e-06, 9.0599e-06, 1.3590e-05],\n",
      "        [6.6757e-06, 1.3351e-05, 2.0027e-05]], grad_fn=<StackBackward0>), tensor([[2.2650e-06]], grad_fn=<StackBackward0>)]\n",
      "tensor([[2.2650e-06, 4.5300e-06, 6.7949e-06, 2.2650e-06],\n",
      "        [4.5300e-06, 9.0599e-06, 1.3590e-05, 4.5300e-06],\n",
      "        [6.6757e-06, 1.3351e-05, 2.0027e-05, 6.6757e-06],\n",
      "        [2.2650e-06, 4.5300e-06, 6.7949e-06, 2.2650e-06]],\n",
      "       grad_fn=<CatBackward0>) tensor(False)\n",
      "tensor([[1., 2., 3., 1.],\n",
      "        [2., 4., 6., 2.],\n",
      "        [3., 6., 9., 3.],\n",
      "        [1., 2., 3., 1.]], grad_fn=<CatBackward0>) tensor(True)\n",
      "tensor([ 3.1352e-05,  9.1229e-13, -9.6906e-15], grad_fn=<SelectBackward0>)\n",
      "tensor([ 8.6018e-05,  5.0651e-13, -7.7167e-13], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "logistic_test_model = LogisticRegression(3, 2)\n",
    "logistic_test_model.linear.weight.data = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "logistic_test_model.linear.bias.data = torch.tensor([-1.0])\n",
    "print(logistic_test_model.linear.weight, logistic_test_model.linear.bias)\n",
    "# logistic_test_model.weight.data = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "# logistic_test_model.bias.data = torch.tensor([1.0])\n",
    "\n",
    "input = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "output = logistic_test_model(input)\n",
    "label = torch.tensor([0])\n",
    "label_float = torch.tensor([0.0])\n",
    "print(output, label)\n",
    "\n",
    "hessian_expected = torch.sigmoid(output[0][0]) * torch.sigmoid(1 - output[0][0]) * (input.T @ input)\n",
    "print(f\"hessain expected: {hessian_expected}, is semi positive definite: {is_semi_positive_definite(hessian_expected)}\")\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(output, label)\n",
    "loss_MSE = torch.nn.MSELoss()(output, label_float)\n",
    "\n",
    "test_hessian = calc_criterion_second_order_derivative(loss, logistic_test_model)\n",
    "test_hessian_list = calc_loss_second_order_derivative_list(loss, logistic_test_model)\n",
    "print(test_hessian, is_semi_positive_definite(test_hessian))\n",
    "test_hessian_MSE = calc_criterion_second_order_derivative(loss_MSE, logistic_test_model)\n",
    "print(test_hessian_MSE, is_semi_positive_definite(test_hessian_MSE))\n",
    "print(torch.linalg.eigvals(test_hessian_list[0]).real)\n",
    "print(torch.linalg.eigvals(hessian_expected).real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009890492626539099"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "exp(-6)/(1+exp(-6))*4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
