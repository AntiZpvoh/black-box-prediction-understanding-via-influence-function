{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transforms.ToTensor(), )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/logistic_regression_10_mnist') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(28*28, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_num = 0\n",
    "for epoch in range(5):\n",
    "    model.train() # Set the model to training mode\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = batch_data.view(-1, 28*28)\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        writer.add_scalar('training loss', loss, batch_num)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "val_loss = 0\n",
    "val_correct = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for data, labels in test_loader:\n",
    "        # Transfer data to the appropriate device (CPU or GPU)\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data.view(-1, 28*28))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Update validation metrics (e.g., accuracy)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        val_correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5635, Validation Accuracy: 91.06%\n"
     ]
    }
   ],
   "source": [
    "val_loss /= len(test_loader)\n",
    "val_accuracy = 100 * val_correct / len(test_dataset)\n",
    "\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample_dataset(dataset, t):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    sampler = torch.utils.data.sampler.RandomSampler(dataset, num_samples=t)\n",
    "    sampled_dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n",
    "    return sampled_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) tensor([7])\n",
      "torch.Size([1, 1, 28, 28]) tensor([1])\n",
      "torch.Size([1, 1, 28, 28]) tensor([4])\n",
      "torch.Size([1, 1, 28, 28]) tensor([8])\n",
      "torch.Size([1, 1, 28, 28]) tensor([9])\n",
      "torch.Size([1, 1, 28, 28]) tensor([7])\n",
      "torch.Size([1, 1, 28, 28]) tensor([6])\n",
      "torch.Size([1, 1, 28, 28]) tensor([2])\n",
      "torch.Size([1, 1, 28, 28]) tensor([3])\n",
      "torch.Size([1, 1, 28, 28]) tensor([3])\n"
     ]
    }
   ],
   "source": [
    "sample_loader = uniform_sample_dataset(train_dataset, 10)\n",
    "for batch_data, batch_labels in sample_loader:\n",
    "    print(batch_data.shape, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_first_order_derivative(data_tensor, label_tensor, criterion, model):\n",
    "    # Set requires_grad to True for the data tensor to enable gradient computation\n",
    "    # data_tensor.requires_grad = True\n",
    "    # label_tensor.requires_grad = True\n",
    "    model.zero_grad()\n",
    "    output = model(data_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    param_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    param_grads = torch.cat(param_grads)\n",
    "    # print(param_grads)\n",
    "    # return torch.concat([data_tensor.grad.reshape(1, -1), label_tensor.grad.reshape(1, -1)], dim=1)\n",
    "    return param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_second_order_derivative(data_tensor, label_tensor, criterion, model):\n",
    "    # Set requires_grad to True for the data tensor to enable gradient computation\n",
    "    # data_tensor.requires_grad = True\n",
    "    # label_tensor.requires_grad = True\n",
    "    # loss = criterion(output, label_tensor)\n",
    "    model.zero_grad()\n",
    "    output = model(data_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    # first_order_derivative.requires_grad = True\n",
    "    second_grads = []\n",
    "    for first_grad in first_grads:\n",
    "        row = []\n",
    "        for p in model.parameters():\n",
    "            sub_matrix = []\n",
    "            # print(first_grad.shape, p.shape)\n",
    "            for i in range(first_grad.shape[0]):\n",
    "                sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "            sub_matrix = torch.stack(sub_matrix)\n",
    "            # print(sub_matrix)\n",
    "            row.append(sub_matrix)\n",
    "        row = torch.cat(row, dim=1)\n",
    "        # print(row)\n",
    "        second_grads.append(row)\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "    # print(\"sec: \", second_grads)\n",
    "    # hessian_matrix = torch.autograd.functional.hessian(lambda x: criterion(model(x), label_tensor), data_tensor).reshape(data_tensor.shape[1], -1)\n",
    "    # hessian_matrix = torch.autograd.functional.hessian(lambda x, y: criterion(model(x), y), (data_tensor, label_tensor))\n",
    "    return second_grads\n",
    "    # matrix_list = []\n",
    "    # for row, var in zip(hessian_matrix, [data_tensor, label_tensor]):\n",
    "    #     list_row = []\n",
    "    #     for tensor in row:\n",
    "    #         if len(var.shape) == 1:\n",
    "    #             list_row.append(tensor.reshape(1, -1))\n",
    "    #         else:\n",
    "    #             list_row.append(tensor.reshape(var.shape[1], -1))\n",
    "    #     matrix_list.append(list_row)\n",
    "    # # Concatenate along the first dimension\n",
    "    # concatenated_hessian = torch.cat([torch.cat(row, dim=1) for row in matrix_list], dim=0)\n",
    "\n",
    "    # return concatenated_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1205,  0.2650]], requires_grad=True) Parameter containing:\n",
      "tensor([0.2224], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_model = torch.nn.Linear(2, 1)\n",
    "print(test_model.weight, test_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7360, -1.4720]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "label_tensor = torch.tensor([1.0])\n",
    "\n",
    "2 * (test_model(data_tensor) - label_tensor).item() * data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7360, -1.4720, -0.7360], grad_fn=<CatBackward0>)\n",
      "tensor([[2., 4., 2.],\n",
      "        [4., 8., 4.],\n",
      "        [2., 4., 2.]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/torch/csrc/autograd/engine.cpp:1182.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "print(calc_criterion_first_order_derivative(data_tensor, label_tensor, torch.nn.MSELoss(), test_model))\n",
    "print(calc_criterion_second_order_derivative(data_tensor, label_tensor, torch.nn.MSELoss(), test_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7850])\n",
      "torch.Size([7850, 7850])\n"
     ]
    }
   ],
   "source": [
    "one_train_dataloader = uniform_sample_dataset(train_dataset, 1)\n",
    "for batch_data, batch_labels in one_train_dataloader:\n",
    "    print(calc_criterion_first_order_derivative(batch_data, batch_labels, criterion, model).shape)\n",
    "    print(calc_criterion_second_order_derivative(batch_data.view(-1, 28*28), batch_labels, criterion, model).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(y, w, v):\n",
    "    # First backprop\n",
    "    first_grads = torch.autograd.grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    first_grads = [g.flatten() for g in first_grads]\n",
    "    first_grads = torch.cat(first_grads, dim=0)\n",
    "    # print(first_grads[0], first_grads[1].sum())\n",
    "    \n",
    "    # Elementwise products\n",
    "    # print(f\"first_grads: {first_grads}, v: {v}, first_grads * v: {first_grads * v}\")\n",
    "    elemwise_products = torch.sum(first_grads * v)\n",
    "    # elemwise_products = 0\n",
    "    # for grad_elem, v_elem in zip(first_grads, v):\n",
    "    #     grad_elem = grad_elem.flatten()\n",
    "    #     elemwise_products += torch.sum(grad_elem * v_elem)\n",
    "\n",
    "    # Second backprop\n",
    "    return_grads = torch.autograd.grad(elemwise_products, w, create_graph=True)\n",
    "    return_grads = [g.flatten() for g in return_grads]\n",
    "    # print(return_grads[0].sum(), return_grads[1].sum())\n",
    "    return_grads = torch.cat(return_grads, dim=0)\n",
    "\n",
    "    return return_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7360, -1.4720, -0.7360], grad_fn=<CatBackward0>)\n",
      "tensor([ -8.8321, -17.6643,  -8.8321], grad_fn=<MvBackward0>) tensor([ -8.8321, -17.6643,  -8.8321], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/98gq3v9n7tq0z5f1l_9wzblh0000gn/T/ipykernel_73177/2127892629.py:5: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403213615/work/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  expected = torch.matmul(calc_criterion_second_order_derivative(data_tensor, label_tensor, torch.nn.MSELoss(), test_model), vector.T)\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in test_model.parameters()]\n",
    "vector = calc_criterion_first_order_derivative(data_tensor, label_tensor, torch.nn.MSELoss(), test_model)\n",
    "print(vector)\n",
    "# vector = torch.tensor([-3.0305, -6.0611, -3.0305])\n",
    "expected = torch.matmul(calc_criterion_second_order_derivative(data_tensor, label_tensor, torch.nn.MSELoss(), test_model), vector.T)\n",
    "vector._grad_fn = None\n",
    "# print(vector)\n",
    "test_model.zero_grad()\n",
    "actual = hvp(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), params, vector)\n",
    "print(expected, actual)\n",
    "assert torch.equal(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvp_summary_writer = SummaryWriter('runs/hvp_sum_summary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def ihvp(train_dataset, test_data, test_label, model, criterion, t, r):\n",
    "    hvp_eval_avg = 0\n",
    "    test_data = test_data\n",
    "    vector = calc_criterion_first_order_derivative(test_data, test_label, criterion, model)\n",
    "    # vector = torch.ones((28*28+1)*10)\n",
    "    print(vector)\n",
    "    for i in range(r):\n",
    "        sampled_train_loader = uniform_sample_dataset(train_dataset, t)\n",
    "        # Step 1. Initialize the evaluation of the Hessian-vector product\n",
    "        hvp_eval = vector\n",
    "        data_number = 0\n",
    "        for data, label in sampled_train_loader:\n",
    "            # print(f\"round={i}, data number={data_number}\")\n",
    "            # data, labels = data.to(device), labels.to(device)\n",
    "            # Step 2. Compute the second order gradient of the loss w.r.t. the model parameters\n",
    "            model.zero_grad()\n",
    "            data_tensor = data.view(-1, 28*28)\n",
    "            params = [p for p in model.parameters()]\n",
    "            # params_vector = torch.cat(params)\n",
    "            hvp_eval._grad_fn = None\n",
    "            # print(f\"hvp_eval: {hvp_eval}\")\n",
    "            return_grads = hvp(criterion(model(data_tensor), label), params, hvp_eval)\n",
    "            # print(f\"return_grads: {return_grads}\")\n",
    "            # print(\"return grads: \", return_grads.shape)\n",
    "            # second_order_grad = calc_criterion_second_order_derivative(data_tensor, label, criterion, model)\n",
    "            # Step 3. Compute the inner product between the gradient and the Hessian-vector product\n",
    "            # print(torch.matmul((torch.eye(second_order_grad.shape[0]) - second_order_grad), hvp_eval.T).T)\n",
    "            # print(torch.eye(second_order_grad.shape[0]), second_order_grad.shape, hvp_eval.shape)\n",
    "            # print(torch.matmul((torch.eye(second_order_grad.shape[0]) - second_order_grad), hvp_eval.T).shape)\n",
    "            # print(hvp_eval.sum())\n",
    "            # product = torch.matmul((torch.eye(second_order_grad.shape[0]) - second_order_grad), hvp_eval.T).T\n",
    "            # print(product)\n",
    "            # for p in range(len(hvp_evals)):\n",
    "            #     print(vectors[p].shape, hvp_evals[p], return_grads[p].shape)\n",
    "            #     hvp_evals[p] = vectors[p] + hvp_evals[p] - return_grads[p]\n",
    "            #     hvp_summary_writer.add_scalar(f'hvp_eval_sum_{i}', hvp_evals[i].sum(), data_number + i * t)\n",
    "            hvp_eval = hvp_eval + vector - return_grads\n",
    "            # print(f\"sum: {hvp_eval.sum()}\")\n",
    "            hvp_summary_writer.add_scalar(f'hvp_eval_sum', hvp_eval.sum(), data_number + i * t)\n",
    "            data_number += 1\n",
    "            gc.collect()\n",
    "        hvp_eval_avg = i / (i + 1) * hvp_eval_avg + 1 / (i + 1) * hvp_eval\n",
    "            \n",
    "    return hvp_eval_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = uniform_sample_dataset(test_dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4918e-06, 4.4457e-04,\n",
      "        1.4795e-05], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10ecc4800>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "hvp_summary_writer = SummaryWriter('runs/hvp_sum_summary') \n",
    "for test_data, test_label in test_dataloader:\n",
    "    hvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion, 5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upweighting_loss_influence_function(train_dataset, upweighted_data, upweighted_label, test_data, test_label, model, criterion):\n",
    "    # Step 1. Compute the Hessian-vector product\n",
    "    hvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion)\n",
    "    # Step 2. Compute the influence function\n",
    "    influence = torch.dot(-hvp_eval, calc_criterion_first_order_derivative(upweighted_data, upweighted_label, criterion, model))\n",
    "    return influence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
