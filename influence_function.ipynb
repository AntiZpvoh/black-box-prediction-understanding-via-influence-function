{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(folder_path)\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist\")\n",
    "\n",
    "delete_folder('./runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_dataset_all = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transforms.ToTensor(), )\n",
    "\n",
    "subset_size = 55000  # The size of the subset I want\n",
    "indices = torch.randperm(len(train_dataset_all))[:subset_size]\n",
    "train_dataset = Subset(train_dataset_all, indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, params=None):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if params:\n",
    "            self.linear.weight = nn.Parameter(params['weight'])\n",
    "            self.linear.bias = nn.Parameter(params['bias'])\n",
    "        # self.linear.weight = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        # self.linear.bias = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        # outputs = torch.nn.functional.softmax(self.linear(x))\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "class LogisticRegressionRestricted(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionRestricted, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim - 1)\n",
    "        # self.linear.weight = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        # self.linear.bias = nn.Parameter(torch.zeros(output_dim - 1,input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.linear.in_features)\n",
    "        # outputs = torch.nn.functional.softmax(self.linear(x))\n",
    "        outputs = self.linear(x)\n",
    "        zeros_for_last_class = torch.zeros(\n",
    "            (outputs.shape[0], 1),\n",
    "            device=x.device,\n",
    "            dtype=x.dtype\n",
    "        )\n",
    "        output_with_zeros = torch.cat((outputs, zeros_for_last_class), dim=1)\n",
    "        return output_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_dataset, writer, t=20, leave_out_indices=None):\n",
    "    batch_num = 0\n",
    "    if leave_out_indices:\n",
    "        train_dataset = Subset(train_dataset, [i for i in range(len(train_dataset)) if i not in leave_out_indices])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    for epoch in range(t):\n",
    "        model.train() # Set the model to training mode\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            batch_data = batch_data.view(-1, 28*28)\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_labels)\n",
    "            \n",
    "            writer.add_scalar('training loss', loss, batch_num)\n",
    "            loss.backward()\n",
    "            optimizer.step(lambda: loss)\n",
    "            batch_num += 1\n",
    "            \n",
    "    return batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, test_dataset):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    incorrect_data_list, incorrect_label_list = [], []\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for data, labels in test_loader:\n",
    "            # Transfer data to the appropriate device (CPU or GPU)\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data.view(-1, 28*28))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update validation metrics (e.g., accuracy)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            incorrect_data_list.append(data[predicted != labels])\n",
    "            incorrect_label_list.append(labels[predicted != labels])\n",
    "            \n",
    "    incorrect_data = torch.cat(incorrect_data_list, dim=0)\n",
    "    incorrect_label = torch.cat(incorrect_label_list, dim=0)\n",
    "    val_loss /= len(test_dataset)\n",
    "    val_accuracy = 100 * val_correct / len(test_dataset)\n",
    "    return val_loss, val_accuracy, incorrect_data, incorrect_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': tensor([[-0.0160, -0.0071, -0.0005,  ...,  0.0163, -0.0246,  0.0103],\n",
      "        [ 0.0106,  0.0180,  0.0217,  ...,  0.0085,  0.0312,  0.0261],\n",
      "        [-0.0100,  0.0209,  0.0121,  ..., -0.0010,  0.0092, -0.0201],\n",
      "        ...,\n",
      "        [ 0.0164,  0.0095, -0.0330,  ..., -0.0157, -0.0188, -0.0186],\n",
      "        [ 0.0119,  0.0315, -0.0328,  ..., -0.0129, -0.0136, -0.0049],\n",
      "        [-0.0169,  0.0244, -0.0212,  ...,  0.0170, -0.0356,  0.0041]]), 'bias': tensor([ 0.0197,  0.0177,  0.0009,  0.0026, -0.0104, -0.0284,  0.0033, -0.0143,\n",
      "         0.0084,  0.0055])}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(28*28, 10)\n",
    "initial_params = {\"weight\": model.linear.weight.data.clone(), \"bias\": model.linear.bias.data.clone()}\n",
    "print(initial_params)\n",
    "optimizer_lbfgs = torch.optim.LBFGS(model.parameters())\n",
    "optimizer_adam = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def criterion_l2(output, target):\n",
    "    loss = criterion(output, target)\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param, 2)**2\n",
    "    loss += 0.01 * l2_reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/logistic_regression_10_mnist') \n",
    "batch_num = train(model, optimizer_adam, criterion, train_dataset, writer, t=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0043, Validation Accuracy: 92.47%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy, incorrect_data, incorrect_label = test(model, criterion, test_dataset)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_index = torch.randint(0, len(incorrect_data), (1,)).item()\n",
    "x_test, y_test = incorrect_data[test_data_index], incorrect_label[test_data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': tensor([[-0.0160, -0.0071, -0.0005,  ...,  0.0163, -0.0246,  0.0103],\n",
      "        [ 0.0106,  0.0180,  0.0217,  ...,  0.0085,  0.0312,  0.0261],\n",
      "        [-0.0100,  0.0209,  0.0121,  ..., -0.0010,  0.0092, -0.0201],\n",
      "        ...,\n",
      "        [ 0.0164,  0.0095, -0.0330,  ..., -0.0157, -0.0188, -0.0186],\n",
      "        [ 0.0119,  0.0315, -0.0328,  ..., -0.0129, -0.0136, -0.0049],\n",
      "        [-0.0169,  0.0244, -0.0212,  ...,  0.0170, -0.0356,  0.0041]]), 'bias': tensor([ 0.0197,  0.0177,  0.0009,  0.0026, -0.0104, -0.0284,  0.0033, -0.0143,\n",
      "         0.0084,  0.0055])}\n"
     ]
    }
   ],
   "source": [
    "print(initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -5.1887,  -6.5113,  -0.4661,   0.5459,  -7.3655,   1.7023,   1.1639,\n",
      "         -17.2236,  -0.2190, -12.3769]], grad_fn=<AddmmBackward0>) tensor([8])\n",
      "tensor(2.6916, grad_fn=<NllLossBackward0>) tensor(2.6916, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "writer_retrain = SummaryWriter('runs/logistic_regression_10_mnist_retrain') \n",
    "\n",
    "print(model(x_test.view(-1, 28*28)), y_test.view(1))\n",
    "loss_z_test_with_z = criterion(model(x_test), y_test.view(1))\n",
    "\n",
    "params = {\"weight\": initial_params[\"weight\"].clone(), \"bias\": initial_params[\"bias\"].clone()}\n",
    "model_retrained = LogisticRegression(28*28, 10, params=params)\n",
    "optimizer_adam_retrained = torch.optim.Adam(model_retrained.parameters())\n",
    "# print(indices)\n",
    "# subset_size = 55000  # The size of the subset I want\n",
    "# indices = torch.randperm(len(train_dataset_all))[:subset_size]\n",
    "# print(indices)\n",
    "# train_dataset_partial = Subset(train_dataset_all, indices)\n",
    "train(model_retrained, optimizer_adam_retrained, criterion, train_dataset, writer_retrain, t=5, leave_out_indices=[])\n",
    "\n",
    "loss_z_test_without_z = criterion(model_retrained(x_test), y_test.view(1))\n",
    "\n",
    "print(loss_z_test_with_z, loss_z_test_without_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_leave_one_out_loss_diff(model, initial_params, criterion, test_data, test_label, leave_out_indices):\n",
    "    params = {\"weight\": initial_params[\"weight\"].clone(), \"bias\": initial_params[\"bias\"].clone()}\n",
    "    loss_z_test_with_z = criterion(model(test_data), test_label.view(1))\n",
    "    model_leave_one_out = LogisticRegression(28*28, 10, params=params)\n",
    "    optimizer_adam_leave_one_out = torch.optim.Adam(model_leave_one_out.parameters())\n",
    "    train(model_leave_one_out, optimizer_adam_leave_one_out, criterion, train_dataset, writer_retrain, t=5, leave_out_indices=leave_out_indices)\n",
    "    loss_z_test_without_z = criterion(model_leave_one_out(test_data), test_label.view(1))\n",
    "    leave_one_out_loss_diff = torch.abs(loss_z_test_with_z - loss_z_test_without_z)\n",
    "    print(f\"loss_z_test_with_z: {loss_z_test_with_z}, loss_z_test_without_z: {loss_z_test_without_z}, leave_one_out_loss_diff: {leave_one_out_loss_diff}\")\n",
    "    return leave_one_out_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_z_test_with_z: 2.6915714740753174, loss_z_test_without_z: 2.6915714740753174, leave_one_out_loss_diff: 0.0\n",
      "tensor(0., grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(calc_leave_one_out_loss_diff(model, initial_params, criterion, x_test, y_test, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': tensor([[-0.0160, -0.0071, -0.0005,  ...,  0.0163, -0.0246,  0.0103],\n",
      "        [ 0.0106,  0.0180,  0.0217,  ...,  0.0085,  0.0312,  0.0261],\n",
      "        [-0.0100,  0.0209,  0.0121,  ..., -0.0010,  0.0092, -0.0201],\n",
      "        ...,\n",
      "        [ 0.0164,  0.0095, -0.0330,  ..., -0.0157, -0.0188, -0.0186],\n",
      "        [ 0.0119,  0.0315, -0.0328,  ..., -0.0129, -0.0136, -0.0049],\n",
      "        [-0.0169,  0.0244, -0.0212,  ...,  0.0170, -0.0356,  0.0041]]), 'bias': tensor([ 0.0197,  0.0177,  0.0009,  0.0026, -0.0104, -0.0284,  0.0033, -0.0143,\n",
      "         0.0084,  0.0055])}\n"
     ]
    }
   ],
   "source": [
    "print(initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0041, Validation Accuracy: 92.81%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy, incorrect_data, incorrect_label = test(model_retrained, criterion, test_dataset)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample_dataset(dataset, t):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    sampler = torch.utils.data.sampler.RandomSampler(dataset, num_samples=t)\n",
    "    sampled_dataloader = torch.utils.data.DataLoader(dataset, batch_size=500, sampler=sampler)\n",
    "    return sampled_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28]) tensor([8, 7, 1, 8, 5, 6, 7, 3, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "sample_loader = uniform_sample_dataset(train_dataset, 10)\n",
    "for batch_data, batch_labels in sample_loader:\n",
    "    print(batch_data.shape, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_first_order_derivative(loss, model):\n",
    "    # Set requires_grad to True for the data tensor to enable gradient computation\n",
    "    # data_tensor.requires_grad = True\n",
    "    # label_tensor.requires_grad = True\n",
    "    model.zero_grad()\n",
    "    # output = model(data_tensor)\n",
    "    # loss = criterion(output, label_tensor)\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    param_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    param_grads = torch.cat(param_grads)\n",
    "    # print(param_grads)\n",
    "    # return torch.concat([data_tensor.grad.reshape(1, -1), label_tensor.grad.reshape(1, -1)], dim=1)\n",
    "    return param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_second_order_derivative_list(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ (p.grad.flatten(), p) for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = []\n",
    "    for first_grad, p in first_grads:\n",
    "        sub_matrix = []\n",
    "        # print(first_grad.shape, p.shape)\n",
    "        for i in range(first_grad.shape[0]):\n",
    "            sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "        sub_matrix = torch.stack(sub_matrix)\n",
    "        second_grads.append(sub_matrix)\n",
    "    # second_order_grads = torch.cat(second_order_grads)\n",
    "    print(second_grads)\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_criterion_second_order_derivative(loss, model):\n",
    "    model.zero_grad()\n",
    "    # Compute the first-order gradient\n",
    "    loss.backward(create_graph=True)\n",
    "    first_grads = [ p.grad.flatten() for p in model.parameters() if p.requires_grad ]\n",
    "    # Compute the second-order gradient\n",
    "    second_grads = []\n",
    "    for first_grad in first_grads:\n",
    "        row = []\n",
    "        for p in model.parameters():\n",
    "            sub_matrix = []\n",
    "            for i in range(first_grad.shape[0]):\n",
    "                sub_matrix.append(torch.autograd.grad(first_grad[i], p, create_graph=True)[0].flatten())\n",
    "            sub_matrix = torch.stack(sub_matrix)\n",
    "            # print(sub_matrix)\n",
    "            row.append(sub_matrix)\n",
    "        row = torch.cat(row, dim=1)\n",
    "        # print(row)\n",
    "        second_grads.append(row)\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.nn.Linear(2, 2)\n",
    "test_model = LogisticRegression(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4469, -1.5358]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data_tensor = torch.tensor([[1.0, -2.0]], requires_grad=True)\n",
    "label_tensor = torch.tensor([1.0, 0.0])\n",
    "label_tensor_long = torch.tensor([0])\n",
    "\n",
    "print(test_model(data_tensor)-label_tensor)\n",
    "# 2 * (test_model(data_tensor) - label_tensor).item() * data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4469, -0.8939, -1.5358,  3.0717,  0.4469, -1.5358],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([[ 0.0459, -0.0918, -0.0459,  0.0918,  0.0459, -0.0459],\n",
      "        [-0.0918,  0.1835,  0.0918, -0.1835, -0.0918,  0.0918],\n",
      "        [-0.0459,  0.0918,  0.0459, -0.0918, -0.0459,  0.0459],\n",
      "        [ 0.0918, -0.1835, -0.0918,  0.1835,  0.0918, -0.0918],\n",
      "        [ 0.0459, -0.0918, -0.0459,  0.0918,  0.0459, -0.0459],\n",
      "        [-0.0459,  0.0918,  0.0459, -0.0918, -0.0459,  0.0459]],\n",
      "       grad_fn=<CatBackward0>) tensor(False)\n",
      "[tensor([[ 1., -2.,  0., -0.],\n",
      "        [-2.,  4.,  0., -0.],\n",
      "        [ 0., -0.,  1., -2.],\n",
      "        [ 0., -0., -2.,  4.]], grad_fn=<StackBackward0>), tensor([[1., 0.],\n",
      "        [0., 1.]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1., -2.,  0., -0.],\n",
       "         [-2.,  4.,  0., -0.],\n",
       "         [ 0., -0.,  1., -2.],\n",
       "         [ 0., -0., -2.,  4.]], grad_fn=<StackBackward0>),\n",
       " tensor([[1., 0.],\n",
       "         [0., 1.]])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(calc_criterion_first_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model))\n",
    "hessian = calc_criterion_second_order_derivative(criterion(test_model(data_tensor), label_tensor_long), test_model)\n",
    "print(hessian, torch.all(torch.linalg.eigvals(hessian).real >= 0))\n",
    "calc_loss_second_order_derivative_list(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7850])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7850, 7850])\n"
     ]
    }
   ],
   "source": [
    "one_train_dataloader = uniform_sample_dataset(train_dataset, 1)\n",
    "for batch_data, batch_labels in one_train_dataloader:\n",
    "    print(calc_criterion_first_order_derivative(criterion(model(batch_data), batch_labels), model).shape)\n",
    "    hessian = calc_criterion_second_order_derivative(torch.nn.MSELoss()(model(batch_data), batch_labels.to(torch.float32)), model)\n",
    "    print(hessian.shape)\n",
    "    # hessian = calc_loss_second_order_derivative_list(criterion(model(batch_data), batch_labels), model)\n",
    "    # hessian_1 = hessian[0]\n",
    "    # hessian_2 = hessian[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = torch.linalg.eigvals(hessian).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(y, w, v):\n",
    "    # First derivative\n",
    "    first_grads = torch.autograd.grad(y, w, retain_graph=True, create_graph=True)\n",
    "\n",
    "    first_grads = [g.flatten() for g in first_grads]\n",
    "    first_grads = torch.cat(first_grads, dim=0)\n",
    "    \n",
    "    # Calculate the element-wise product between the first gradients and the vector v\n",
    "    elemwise_products = torch.sum(first_grads * v)\n",
    "\n",
    "    # Second derivative\n",
    "    second_grads = torch.autograd.grad(elemwise_products, w, create_graph=True)\n",
    "    second_grads = [g.flatten() for g in second_grads]\n",
    "    second_grads = torch.cat(second_grads, dim=0)\n",
    "\n",
    "    return second_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4469, -0.8939, -1.5358,  3.0717,  0.4469, -1.5358],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([ 2.6817, -5.3633, -9.2151, 18.4302,  2.6817, -9.2151],\n",
      "       grad_fn=<MvBackward0>) tensor([ 2.6817, -5.3633, -9.2151, 18.4302,  2.6817, -9.2151],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in test_model.parameters()]\n",
    "vector = calc_criterion_first_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model)\n",
    "print(vector)\n",
    "# vector = torch.tensor([-3.0305, -6.0611, -3.0305])\n",
    "expected = torch.matmul(calc_criterion_second_order_derivative(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), test_model), vector.T)\n",
    "vector._grad_fn = None\n",
    "# print(vector)\n",
    "test_model.zero_grad()\n",
    "actual = hvp(torch.nn.MSELoss()(test_model(data_tensor), label_tensor), params, vector)\n",
    "print(expected, actual)\n",
    "assert torch.equal(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hessian_eigvals = None\n",
    "def ihvp(train_dataset, test_data, test_label, model, criterion, t, r, reg_factor=0.91, scale=0.04, unique_datapoint=None, ihvp_summary_writer=SummaryWriter('runs/ihvp_sum_summary')):\n",
    "    ihvp_eval_avg = 0\n",
    "    test_data = test_data\n",
    "    def scaled_criterion(output, label):\n",
    "        return criterion(output, label) * scale\n",
    "    \n",
    "    vector = calc_criterion_first_order_derivative(scaled_criterion(model(test_data), test_label), model)\n",
    "    for i in range(r):\n",
    "        if unique_datapoint is None:\n",
    "            sampled_train_dataset = [(data, label) for data, label in uniform_sample_dataset(train_dataset, t)]\n",
    "        else:\n",
    "            sampled_train_dataset = []\n",
    "            for _ in range(t):\n",
    "                sampled_train_dataset.append(unique_datapoint)\n",
    "        # Step 1. Initialize the evaluation of the Hessian-vector product\n",
    "        ihvp_eval = vector\n",
    "        data_number = 0\n",
    "        for data, label in sampled_train_dataset:\n",
    "            # Step 2. Compute the second order gradient of the loss w.r.t. the model parameters\n",
    "            model.zero_grad()\n",
    "            data_tensor = data.view(-1, 28*28)\n",
    "            params = [p for p in model.parameters()]\n",
    "            ihvp_eval._grad_fn = None\n",
    "            # Step 3. Compute the inner product between the Hessian matrix and the test gradient vector using HVP\n",
    "            second_grads = hvp(scaled_criterion(model(data_tensor), label), params, ihvp_eval)\n",
    "            # hessian = calc_criterion_second_order_derivative(scaled_criterion(model(data_tensor), label), model)\n",
    "            # hessian_list = calc_loss_second_order_derivative_list(scaled_criterion(model(data_tensor), label), model)\n",
    "            # hessian_eigvals = torch.linalg.eigvals(hessian).real\n",
    "            # # diff = torch.norm(torch.eye(hessian.shape[0])-hessian, p=2)\n",
    "            # print(f\"is semi positive definite: {torch.all(hessian_eigvals >= 0)}\")\n",
    "            # print(f\"hessain eigvals: {torch.min(hessian_eigvals)}\")\n",
    "            # return hessian_eigvals\n",
    "            # print(f\"diff: {diff}, l2 for I: {torch.norm(torch.eye(hessian.shape[0]), p=2)}, hes: {hessian.sum()}, eig: {torch.max(torch.abs(torch.linalg.eigvals(hessian)))}\")\n",
    "            # return_grads_validation = torch.matmul(hessian, ihvp_eval.T)\n",
    "            # print(f\"diff: {torch.abs(return_grads.sum() - vector.sum())}, ihvp_eval: {ihvp_eval.sum()}, return_grads: {return_grads.sum()}, vector: {vector.sum()}\")\n",
    "            # print(f\"validation: {return_grads_validation.sum()}, hessian: {hessian.sum()}\")\n",
    "            ihvp_eval = (1 - reg_factor) * ihvp_eval + vector - second_grads\n",
    "            ihvp_summary_writer.add_scalar(f'ihvp_eval_sum_{i}', ihvp_eval.sum(), data_number)\n",
    "            data_number += 1\n",
    "        # print(f\"ihvp iteration {i} done and ihvp sum is {ihvp_eval.sum()}\")\n",
    "        ihvp_eval_avg = i / (i + 1) * ihvp_eval_avg + 1 / (i + 1) * ihvp_eval         \n",
    "    return ihvp_eval_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = uniform_sample_dataset(test_dataset, 1)\n",
    "test_data_list = [(data, label) for data, label in test_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data_loader  = uniform_sample_dataset(train_dataset, 1)\n",
    "uni_data_tuple = ()\n",
    "for data, label in uni_data_loader:\n",
    "    uni_data_tuple = (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data value sum: 91.50196075439453, Test data label: tensor([8])\n"
     ]
    }
   ],
   "source": [
    "ihvp_summary_writer = SummaryWriter('runs/ihvp_sum_summary') \n",
    "for test_data, test_label in test_data_list:\n",
    "    print(f\"Test data value sum: {test_data.sum()}, Test data label: {test_label}\")\n",
    "    model.zero_grad()\n",
    "    ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion, 5000, 10, reg_factor=0.96, scale=0.01, ihvp_summary_writer=ihvp_summary_writer, unique_datapoint=uni_data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.1118e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihvp_eval.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upweighting_loss_influence_function(train_dataset, upweighted_data, upweighted_label, test_data, test_label, model, criterion):\n",
    "    # Step 1. Compute the Hessian-vector product\n",
    "    ihvp_eval = ihvp(train_dataset, test_data, test_label, model, criterion)\n",
    "    # Step 2. Compute the influence function\n",
    "    influence = torch.dot(-ihvp_eval, calc_criterion_first_order_derivative(upweighted_data, upweighted_label, criterion, model))\n",
    "    return influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss difference of leave-one-out retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0016227766016836\n",
      "[[-400.  300.]\n",
      " [ 300. -200.]]\n",
      "[[ 77.70247895 -44.32744073]\n",
      " [-44.32744073  48.1508518 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "I = np.identity(2)\n",
    "A = np.array([[2e-2,3e-2],[3e-2,4e-2]])\n",
    "\n",
    "print(np.linalg.norm(I - A, ord=2))\n",
    "A_inv = I\n",
    "\n",
    "A_inv_ = np.linalg.inv(A)\n",
    "\n",
    "for i in range(100):\n",
    "    A_inv = A_inv + I - A@A_inv\n",
    "    \n",
    "print(A_inv_)\n",
    "print(A_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_semi_positive_definite(matrix):\n",
    "    return torch.all(torch.linalg.eigvals(matrix).real >= -1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 2.]], requires_grad=True) Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([[4.]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "hessain expected: tensor([[0.0177, 0.0353],\n",
      "        [0.0353, 0.0707]], grad_fn=<MulBackward0>), is semi positive definite: True\n",
      "tensor([[1.]], grad_fn=<SoftmaxBackward0>)\n",
      "[tensor([[0., 0.],\n",
      "        [0., 0.]], grad_fn=<StackBackward0>), tensor([[0.]], grad_fn=<StackBackward0>)]\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<CatBackward0>) tensor(True)\n",
      "test_hessian_MSE: tensor([[2., 4., 2.],\n",
      "        [4., 8., 4.],\n",
      "        [2., 4., 2.]], grad_fn=<CatBackward0>), is_semi_positive_definite: True\n",
      "tensor([0., 0., 0.], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zpvoh/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "logistic_test_model = LogisticRegression(2, 2)\n",
    "logistic_test_model.linear.weight.data = torch.tensor([[1.0, 2.0]])\n",
    "logistic_test_model.linear.bias.data = torch.tensor([-1.0])\n",
    "print(logistic_test_model.linear.weight, logistic_test_model.linear.bias)\n",
    "# logistic_test_model.weight.data = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "# logistic_test_model.bias.data = torch.tensor([1.0])\n",
    "\n",
    "input = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "output = logistic_test_model(input)\n",
    "label = torch.tensor([0])\n",
    "label_float = torch.tensor([1.0, 0.0])\n",
    "print(output, label)\n",
    "\n",
    "hessian_expected = torch.sigmoid(output[0][0]) * (1-torch.sigmoid(output[0][0])) * (input.T @ input)\n",
    "print(f\"hessain expected: {hessian_expected}, is semi positive definite: {is_semi_positive_definite(hessian_expected)}\")\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(output, label) * 1000\n",
    "loss_MSE = torch.nn.MSELoss()(output, label_float)\n",
    "\n",
    "# jaccobian = calc_criterion_first_order_derivative(loss, logistic_test_model)\n",
    "print(torch.softmax(output, dim=1))\n",
    "# print(jaccobian)\n",
    "\n",
    "test_hessian = calc_criterion_second_order_derivative(loss, logistic_test_model)\n",
    "test_hessian_list = calc_loss_second_order_derivative_list(loss, logistic_test_model)\n",
    "print(test_hessian, is_semi_positive_definite(test_hessian))\n",
    "test_hessian_MSE = calc_criterion_second_order_derivative(loss_MSE, logistic_test_model)\n",
    "print(f\"test_hessian_MSE: {test_hessian_MSE}, is_semi_positive_definite: {is_semi_positive_definite(test_hessian_MSE)}\")\n",
    "# print(torch.linalg.eigvals(test_hessian_list[0]).real)\n",
    "print(torch.linalg.eigvals(test_hessian).real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
